{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.dplm import DiffusionProteinLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def recursive_to(obj, device):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        if device == \"cpu\":\n",
    "            return obj.cpu()\n",
    "        try:\n",
    "            return obj.cuda(device=device, non_blocking=True)\n",
    "        except RuntimeError:\n",
    "            return obj.to(device)\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_to(o, device=device) for o in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(recursive_to(o, device=device) for o in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: recursive_to(v, device=device) for k, v in obj.items()}\n",
    "\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def format_check(args):\n",
    "    seq_list = args.cond_seq\n",
    "    cond_position = args.cond_position\n",
    "    assert len(seq_list) == len(\n",
    "        cond_position\n",
    "    ), \"The length of cond_seq and cond_position does not match.\"\n",
    "    position_list = []\n",
    "    for pos in cond_position:\n",
    "        pos = pos.split(\"-\")\n",
    "        assert (\n",
    "            len(pos) == 2\n",
    "        ), \"The format of position is illegal, which is not correctly splited by '-'\"\n",
    "        start_pos, end_pos = int(pos[0]), int(pos[1])\n",
    "        assert end_pos >= start_pos, \"The end position is smaller than start position.\"\n",
    "        position_list.append((start_pos, end_pos))\n",
    "    # check if position segment has overlap\n",
    "    temp_position_list = [pos for tup in position_list for pos in tup]\n",
    "    for i in range(1, len(temp_position_list) - 2, 2):\n",
    "        assert (\n",
    "            temp_position_list[i + 1] > temp_position_list[i]\n",
    "        ), \"The position segment has overlap, which is not supported\"\n",
    "    # check if the length of each position segment and seq segment matches\n",
    "    for i, (start_pos, end_pos) in enumerate(position_list):\n",
    "        assert len(seq_list[i]) == (\n",
    "            end_pos - start_pos + 1\n",
    "        ), \"The length of each position segment and seq segment does not match.\"\n",
    "    return seq_list, position_list\n",
    "\n",
    "\n",
    "\n",
    "def get_initial(num_seqs, length, tokenizer, device):\n",
    "    seq = [\"<mask>\"] * length\n",
    "    seq = [\"\".join(seq)]\n",
    "    init_seq = seq * num_seqs\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        init_seq, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\"\n",
    "    )\n",
    "    batch = {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"input_mask\": batch[\"attention_mask\"].bool(),\n",
    "    }\n",
    "    # if cond_seq is None:\n",
    "    #     batch['input_ids'], _ = _full_mask(batch['input_ids'].clone(), collater.alphabet)\n",
    "    batch = recursive_to(batch, device)\n",
    "    pprint(batch)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2f5c48215149c88b317f4b5fcc38ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Logan\\.cache\\huggingface\\hub\\models--airkingbd--dplm_150m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036de4a13c3845b882f110050bdae47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/595M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c33379af634caca807529663b81268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13787b45753a47b598df75ef3edcc2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4ebd1e2d3e4259a3cdf273f2646c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f75142051442d0a52cb266ce52d3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/595M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"airkingbd/dplm_150m\"\n",
    "model = DiffusionProteinLanguageModel.from_pretrained(model_name)\n",
    "tokenizer = model.tokenizer\n",
    "model = model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\AppData\\Local\\Temp\\ipykernel_21180\\888801086.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 0, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "         32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "         32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "         32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,  2],\n",
      "        [ 0, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "         32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "         32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "         32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,  2]], device='cuda:0'),\n",
      " 'input_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch = get_initial(2, 64, tokenizer, device)\n",
    "batch['input_ids'][:, 5:20] = tokenizer.encode('A', add_special_tokens=False)[0]\n",
    "partial_mask = batch[\"input_ids\"].ne(model.mask_id).type_as(batch[\"input_mask\"])\n",
    "with torch.cuda.amp.autocast():\n",
    "    outputs = model.generate(\n",
    "        batch=batch,\n",
    "        tokenizer=tokenizer,\n",
    "        max_iter=1,\n",
    "        sampling_strategy='gumbel_argmax',\n",
    "        partial_masks=partial_mask,\n",
    "        disable_resample='False',\n",
    "        resample_ratio=0.25,\n",
    "        temperature=1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 20,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "          5,  5,  5,  5, 16,  5, 13, 11,  5,  5,  5,  5, 10,  5,  9,  6, 13, 11,\n",
       "          5,  5, 17,  5, 11,  5, 21,  5, 16, 10,  4, 23,  8,  9,  6,  5, 15,  8,\n",
       "          8, 12,  5,  9, 16, 14, 14, 23, 11, 14, 18,  2],\n",
       "        [ 0,  6,  7,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "          5,  5,  5, 11,  5,  5,  5,  5,  5, 21, 10,  5,  5,  4,  5,  8, 16,  9,\n",
       "          5,  9, 15,  5,  5,  9, 11,  5,  5, 11,  5,  7,  4, 13,  7,  5, 12,  6,\n",
       "         19, 11, 10, 22, 22, 15,  6, 15,  8, 23, 12,  2]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls> M A R A A A A A A A A A A A A A A A A A A A A A A A T A A T A F G S T A A F A C S T L S T G R M S A H P E P D T T R R T I Q S V <eos>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42345a70ad4740a0acb2231dc43f9c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Logan\\.cache\\huggingface\\hub\\models--Rostlab--prot_t5_xl_half_uniref50-enc. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d555396fbdab4dfc8044ffd0f602201f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at Rostlab/prot_t5_xl_half_uniref50-enc and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2818830336"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc')\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2818.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters()) // 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS4 natural matching characters: 99967/115123 (86.83%)\n",
      "SS9 natural matching characters: 89616/115123 (77.84%)\n",
      "SS4 generated matching characters: 103148/119379 (86.40%)\n",
      "SS9 generated matching characters: 92314/119379 (77.33%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('evaluation/test_compare_ss_pred.csv')\n",
    "\n",
    "# Calculate matching characters between natural and generated sequences for SS4\n",
    "matching_ss4 = 0\n",
    "total_chars = 0\n",
    "for true, pred in zip(df['nat-ss4'], df['nat_ss4']):\n",
    "    # Ensure we only compare up to the length of the shorter string\n",
    "    min_len = min(len(true), len(pred))\n",
    "    total_chars += min_len\n",
    "    # Count matching characters at each position\n",
    "    for i in range(min_len):\n",
    "        if true[i] == pred[i]:\n",
    "            matching_ss4 += 1\n",
    "\n",
    "print(f\"SS4 natural matching characters: {matching_ss4}/{total_chars} ({matching_ss4/total_chars*100:.2f}%)\")\n",
    "\n",
    "# Do the same for SS9\n",
    "matching_ss9 = 0\n",
    "total_chars_ss9 = 0\n",
    "for true, pred in zip(df['nat-ss9'], df['nat_ss9']):\n",
    "    min_len = min(len(true), len(pred))\n",
    "    total_chars_ss9 += min_len\n",
    "    for i in range(min_len):\n",
    "        if true[i] == pred[i]:\n",
    "            matching_ss9 += 1\n",
    "\n",
    "print(f\"SS9 natural matching characters: {matching_ss9}/{total_chars_ss9} ({matching_ss9/total_chars_ss9*100:.2f}%)\")\n",
    "\n",
    "matching_ss4_gen = 0\n",
    "total_chars_ss4_gen = 0\n",
    "for true, pred in zip(df['gen-ss4'], df['gen_ss4']):\n",
    "    min_len = min(len(true), len(pred))\n",
    "    total_chars_ss4_gen += min_len\n",
    "    for i in range(min_len):\n",
    "        if true[i] == pred[i]:\n",
    "            matching_ss4_gen += 1\n",
    "\n",
    "print(f\"SS4 generated matching characters: {matching_ss4_gen}/{total_chars_ss4_gen} ({matching_ss4_gen/total_chars_ss4_gen*100:.2f}%)\")\n",
    "\n",
    "matching_ss9_gen = 0\n",
    "total_chars_ss9_gen = 0\n",
    "for true, pred in zip(df['gen-ss9'], df['gen_ss9']):\n",
    "    min_len = min(len(true), len(pred))\n",
    "    total_chars_ss9_gen += min_len\n",
    "    for i in range(min_len):\n",
    "        if true[i] == pred[i]:\n",
    "            matching_ss9_gen += 1\n",
    "\n",
    "print(f\"SS9 generated matching characters: {matching_ss9_gen}/{total_chars_ss9_gen} ({matching_ss9_gen/total_chars_ss9_gen*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seqs', 'labels'],\n",
       "        num_rows: 10792\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['seqs', 'labels'],\n",
       "        num_rows: 626\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seqs', 'labels'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset('GleghornLab/SS8')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B', 'C', 'D', 'E', 'G', 'H', 'I', 'S', 'T'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for label in data['train']['labels']:\n",
    "    vocab.update(label)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'].to_pandas().to_csv('ss8_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from huggingface_hub import hf_hub_download\n",
    "from models.modeling_esm_diff import ESM_Diff_Binders, ESMDiffConfig\n",
    "from models.utils import wrap_lora\n",
    "\n",
    "MODEL_PATH = 'lhallee/esm_diff_bind_150'\n",
    "base_path = 'GleghornLab/esm_diff_150'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 856 parameters\n",
      "Missing 0 parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoraModel(\n",
       "  (model): ESM_Diff_Binders(\n",
       "    (esm): FAST_ESM_ENCODER(\n",
       "      (embeddings): EsmEmbeddings(\n",
       "        (word_embeddings): Embedding(33, 640, padding_idx=1)\n",
       "      )\n",
       "      (encoder): EsmEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-29): 30 x EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (key): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (value): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): lora.Linear(\n",
       "                (base_layer): Linear(in_features=640, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (contact_head): EsmContactPredictionHead(\n",
       "        (regression): Linear(in_features=600, out_features=1, bias=True)\n",
       "        (activation): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (pooler): EsmPooler(\n",
       "      (dense): lora.Linear(\n",
       "        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (lm_head): LMHead(\n",
       "      (dense): lora.Linear(\n",
       "        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=640, out_features=33, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (ce_loss): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "local_weight_file = hf_hub_download(\n",
    "    repo_id=MODEL_PATH,\n",
    "    filename='model.safetensors',\n",
    "    repo_type='model',\n",
    ")\n",
    "\n",
    "config = ESMDiffConfig.from_pretrained(MODEL_PATH)\n",
    "model = ESM_Diff_Binders(config=config)\n",
    "model = wrap_lora(model, r=config.lora_r, lora_alpha=config.lora_alpha, lora_dropout=config.lora_dropout)\n",
    "state_dict = load_file(local_weight_file)\n",
    "\n",
    "# Track which parameters were loaded\n",
    "loaded_params = set()\n",
    "missing_params = set()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    found = False\n",
    "    for key in state_dict.keys():\n",
    "        if key in name:\n",
    "            param.data = state_dict[key]\n",
    "            loaded_params.add(name)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        missing_params.add(name)\n",
    "\n",
    "# Verify all weights were loaded correctly\n",
    "print(f\"Loaded {len(loaded_params)} parameters\")\n",
    "print(f\"Missing {len(missing_params)} parameters\")\n",
    "if missing_params:\n",
    "    print(\"Missing parameters:\")\n",
    "    for param in sorted(missing_params):\n",
    "        print(f\"  - {param}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.esm.embeddings.word_embeddings.weight\n",
      "model.esm.encoder.layer.0.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.0.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.0.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.0.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.0.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.0.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.0.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.0.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.0.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.0.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.0.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.0.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.LayerNorm.weight\n",
      "model.esm.encoder.layer.0.LayerNorm.bias\n",
      "model.esm.encoder.layer.1.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.1.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.1.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.1.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.1.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.1.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.1.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.1.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.1.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.1.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.1.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.1.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.LayerNorm.weight\n",
      "model.esm.encoder.layer.1.LayerNorm.bias\n",
      "model.esm.encoder.layer.2.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.2.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.2.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.2.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.2.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.2.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.2.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.2.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.2.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.2.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.2.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.2.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.LayerNorm.weight\n",
      "model.esm.encoder.layer.2.LayerNorm.bias\n",
      "model.esm.encoder.layer.3.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.3.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.3.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.3.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.3.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.3.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.3.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.3.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.3.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.3.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.3.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.3.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.LayerNorm.weight\n",
      "model.esm.encoder.layer.3.LayerNorm.bias\n",
      "model.esm.encoder.layer.4.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.4.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.4.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.4.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.4.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.4.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.4.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.4.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.4.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.4.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.4.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.4.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.LayerNorm.weight\n",
      "model.esm.encoder.layer.4.LayerNorm.bias\n",
      "model.esm.encoder.layer.5.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.5.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.5.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.5.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.5.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.5.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.5.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.5.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.5.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.5.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.5.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.5.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.LayerNorm.weight\n",
      "model.esm.encoder.layer.5.LayerNorm.bias\n",
      "model.esm.encoder.layer.6.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.6.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.6.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.6.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.6.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.6.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.6.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.6.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.6.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.6.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.6.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.6.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.6.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.LayerNorm.weight\n",
      "model.esm.encoder.layer.6.LayerNorm.bias\n",
      "model.esm.encoder.layer.7.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.7.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.7.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.7.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.7.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.7.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.7.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.7.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.7.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.7.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.7.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.7.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.7.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.LayerNorm.weight\n",
      "model.esm.encoder.layer.7.LayerNorm.bias\n",
      "model.esm.encoder.layer.8.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.8.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.8.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.8.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.8.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.8.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.8.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.8.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.8.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.8.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.8.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.8.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.8.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.LayerNorm.weight\n",
      "model.esm.encoder.layer.8.LayerNorm.bias\n",
      "model.esm.encoder.layer.9.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.9.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.9.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.9.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.9.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.9.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.9.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.9.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.9.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.9.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.9.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.9.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.9.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.LayerNorm.weight\n",
      "model.esm.encoder.layer.9.LayerNorm.bias\n",
      "model.esm.encoder.layer.10.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.10.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.10.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.10.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.10.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.10.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.10.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.10.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.10.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.10.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.10.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.10.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.10.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.LayerNorm.weight\n",
      "model.esm.encoder.layer.10.LayerNorm.bias\n",
      "model.esm.encoder.layer.11.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.11.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.11.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.11.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.11.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.11.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.11.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.11.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.11.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.11.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.11.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.11.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.11.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.LayerNorm.weight\n",
      "model.esm.encoder.layer.11.LayerNorm.bias\n",
      "model.esm.encoder.layer.12.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.12.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.12.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.12.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.12.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.12.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.12.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.12.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.12.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.12.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.12.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.12.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.12.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.12.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.12.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.12.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.LayerNorm.weight\n",
      "model.esm.encoder.layer.12.LayerNorm.bias\n",
      "model.esm.encoder.layer.13.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.13.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.13.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.13.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.13.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.13.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.13.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.13.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.13.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.13.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.13.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.13.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.13.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.13.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.13.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.13.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.LayerNorm.weight\n",
      "model.esm.encoder.layer.13.LayerNorm.bias\n",
      "model.esm.encoder.layer.14.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.14.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.14.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.14.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.14.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.14.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.14.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.14.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.14.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.14.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.14.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.14.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.14.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.14.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.14.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.14.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.LayerNorm.weight\n",
      "model.esm.encoder.layer.14.LayerNorm.bias\n",
      "model.esm.encoder.layer.15.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.15.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.15.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.15.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.15.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.15.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.15.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.15.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.15.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.15.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.15.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.15.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.15.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.15.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.15.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.15.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.LayerNorm.weight\n",
      "model.esm.encoder.layer.15.LayerNorm.bias\n",
      "model.esm.encoder.layer.16.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.16.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.16.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.16.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.16.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.16.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.16.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.16.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.16.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.16.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.16.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.16.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.16.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.16.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.16.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.16.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.LayerNorm.weight\n",
      "model.esm.encoder.layer.16.LayerNorm.bias\n",
      "model.esm.encoder.layer.17.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.17.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.17.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.17.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.17.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.17.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.17.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.17.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.17.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.17.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.17.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.17.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.17.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.17.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.17.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.17.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.LayerNorm.weight\n",
      "model.esm.encoder.layer.17.LayerNorm.bias\n",
      "model.esm.encoder.layer.18.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.18.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.18.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.18.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.18.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.18.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.18.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.18.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.18.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.18.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.18.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.18.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.18.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.18.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.18.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.18.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.LayerNorm.weight\n",
      "model.esm.encoder.layer.18.LayerNorm.bias\n",
      "model.esm.encoder.layer.19.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.19.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.19.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.19.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.19.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.19.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.19.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.19.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.19.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.19.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.19.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.19.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.19.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.19.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.19.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.19.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.LayerNorm.weight\n",
      "model.esm.encoder.layer.19.LayerNorm.bias\n",
      "model.esm.encoder.layer.20.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.20.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.20.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.20.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.20.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.20.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.20.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.20.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.20.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.20.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.20.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.20.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.20.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.20.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.20.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.20.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.LayerNorm.weight\n",
      "model.esm.encoder.layer.20.LayerNorm.bias\n",
      "model.esm.encoder.layer.21.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.21.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.21.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.21.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.21.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.21.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.21.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.21.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.21.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.21.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.21.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.21.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.21.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.21.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.21.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.21.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.LayerNorm.weight\n",
      "model.esm.encoder.layer.21.LayerNorm.bias\n",
      "model.esm.encoder.layer.22.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.22.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.22.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.22.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.22.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.22.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.22.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.22.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.22.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.22.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.22.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.22.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.22.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.22.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.22.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.22.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.LayerNorm.weight\n",
      "model.esm.encoder.layer.22.LayerNorm.bias\n",
      "model.esm.encoder.layer.23.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.23.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.23.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.23.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.23.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.23.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.23.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.23.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.23.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.23.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.23.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.23.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.23.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.23.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.23.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.23.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.LayerNorm.weight\n",
      "model.esm.encoder.layer.23.LayerNorm.bias\n",
      "model.esm.encoder.layer.24.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.24.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.24.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.24.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.24.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.24.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.24.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.24.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.24.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.24.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.24.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.24.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.24.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.24.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.24.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.24.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.LayerNorm.weight\n",
      "model.esm.encoder.layer.24.LayerNorm.bias\n",
      "model.esm.encoder.layer.25.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.25.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.25.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.25.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.25.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.25.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.25.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.25.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.25.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.25.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.25.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.25.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.25.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.25.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.25.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.25.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.LayerNorm.weight\n",
      "model.esm.encoder.layer.25.LayerNorm.bias\n",
      "model.esm.encoder.layer.26.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.26.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.26.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.26.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.26.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.26.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.26.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.26.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.26.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.26.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.26.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.26.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.26.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.26.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.26.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.26.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.LayerNorm.weight\n",
      "model.esm.encoder.layer.26.LayerNorm.bias\n",
      "model.esm.encoder.layer.27.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.27.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.27.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.27.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.27.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.27.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.27.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.27.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.27.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.27.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.27.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.27.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.27.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.27.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.27.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.27.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.LayerNorm.weight\n",
      "model.esm.encoder.layer.27.LayerNorm.bias\n",
      "model.esm.encoder.layer.28.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.28.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.28.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.28.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.28.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.28.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.28.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.28.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.28.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.28.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.28.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.28.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.28.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.28.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.28.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.28.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.LayerNorm.weight\n",
      "model.esm.encoder.layer.28.LayerNorm.bias\n",
      "model.esm.encoder.layer.29.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.29.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.29.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.29.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.29.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.29.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.29.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.29.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.29.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.29.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.29.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.29.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.29.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.29.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.29.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.29.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.LayerNorm.weight\n",
      "model.esm.encoder.layer.29.LayerNorm.bias\n",
      "model.esm.encoder.emb_layer_norm_after.weight\n",
      "model.esm.encoder.emb_layer_norm_after.bias\n",
      "model.esm.contact_head.regression.weight\n",
      "model.esm.contact_head.regression.bias\n",
      "model.pooler.dense.base_layer.weight\n",
      "model.pooler.dense.base_layer.bias\n",
      "model.pooler.dense.lora_A.default.weight\n",
      "model.pooler.dense.lora_B.default.weight\n",
      "model.lm_head.dense.base_layer.weight\n",
      "model.lm_head.dense.base_layer.bias\n",
      "model.lm_head.dense.lora_A.default.weight\n",
      "model.lm_head.dense.lora_B.default.weight\n",
      "model.lm_head.layer_norm.weight\n",
      "model.lm_head.layer_norm.bias\n",
      "model.lm_head.decoder.weight\n",
      "model.lm_head.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm.contact_head.regression.bias\n",
      "esm.contact_head.regression.weight\n",
      "esm.embeddings.word_embeddings.weight\n",
      "esm.encoder.emb_layer_norm_after.bias\n",
      "esm.encoder.emb_layer_norm_after.weight\n",
      "esm.encoder.layer.0.LayerNorm.bias\n",
      "esm.encoder.layer.0.LayerNorm.weight\n",
      "esm.encoder.layer.0.attention.LayerNorm.bias\n",
      "esm.encoder.layer.0.attention.LayerNorm.weight\n",
      "esm.encoder.layer.0.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.0.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.0.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.0.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.0.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.0.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.0.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.0.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.0.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.0.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.0.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.0.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.0.output.dense.base_layer.bias\n",
      "esm.encoder.layer.0.output.dense.base_layer.weight\n",
      "esm.encoder.layer.0.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.0.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.1.LayerNorm.bias\n",
      "esm.encoder.layer.1.LayerNorm.weight\n",
      "esm.encoder.layer.1.attention.LayerNorm.bias\n",
      "esm.encoder.layer.1.attention.LayerNorm.weight\n",
      "esm.encoder.layer.1.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.1.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.1.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.1.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.1.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.1.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.1.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.1.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.1.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.1.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.1.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.1.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.1.output.dense.base_layer.bias\n",
      "esm.encoder.layer.1.output.dense.base_layer.weight\n",
      "esm.encoder.layer.1.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.1.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.10.LayerNorm.bias\n",
      "esm.encoder.layer.10.LayerNorm.weight\n",
      "esm.encoder.layer.10.attention.LayerNorm.bias\n",
      "esm.encoder.layer.10.attention.LayerNorm.weight\n",
      "esm.encoder.layer.10.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.10.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.10.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.10.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.10.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.10.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.10.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.10.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.10.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.10.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.10.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.10.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.10.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.10.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.10.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.10.output.dense.base_layer.bias\n",
      "esm.encoder.layer.10.output.dense.base_layer.weight\n",
      "esm.encoder.layer.10.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.10.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.11.LayerNorm.bias\n",
      "esm.encoder.layer.11.LayerNorm.weight\n",
      "esm.encoder.layer.11.attention.LayerNorm.bias\n",
      "esm.encoder.layer.11.attention.LayerNorm.weight\n",
      "esm.encoder.layer.11.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.11.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.11.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.11.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.11.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.11.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.11.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.11.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.11.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.11.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.11.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.11.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.11.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.11.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.11.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.11.output.dense.base_layer.bias\n",
      "esm.encoder.layer.11.output.dense.base_layer.weight\n",
      "esm.encoder.layer.11.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.11.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.12.LayerNorm.bias\n",
      "esm.encoder.layer.12.LayerNorm.weight\n",
      "esm.encoder.layer.12.attention.LayerNorm.bias\n",
      "esm.encoder.layer.12.attention.LayerNorm.weight\n",
      "esm.encoder.layer.12.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.12.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.12.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.12.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.12.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.12.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.12.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.12.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.12.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.12.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.12.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.12.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.12.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.12.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.12.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.12.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.12.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.12.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.12.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.12.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.12.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.12.output.dense.base_layer.bias\n",
      "esm.encoder.layer.12.output.dense.base_layer.weight\n",
      "esm.encoder.layer.12.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.12.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.13.LayerNorm.bias\n",
      "esm.encoder.layer.13.LayerNorm.weight\n",
      "esm.encoder.layer.13.attention.LayerNorm.bias\n",
      "esm.encoder.layer.13.attention.LayerNorm.weight\n",
      "esm.encoder.layer.13.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.13.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.13.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.13.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.13.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.13.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.13.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.13.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.13.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.13.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.13.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.13.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.13.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.13.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.13.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.13.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.13.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.13.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.13.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.13.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.13.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.13.output.dense.base_layer.bias\n",
      "esm.encoder.layer.13.output.dense.base_layer.weight\n",
      "esm.encoder.layer.13.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.13.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.14.LayerNorm.bias\n",
      "esm.encoder.layer.14.LayerNorm.weight\n",
      "esm.encoder.layer.14.attention.LayerNorm.bias\n",
      "esm.encoder.layer.14.attention.LayerNorm.weight\n",
      "esm.encoder.layer.14.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.14.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.14.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.14.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.14.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.14.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.14.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.14.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.14.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.14.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.14.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.14.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.14.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.14.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.14.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.14.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.14.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.14.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.14.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.14.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.14.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.14.output.dense.base_layer.bias\n",
      "esm.encoder.layer.14.output.dense.base_layer.weight\n",
      "esm.encoder.layer.14.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.14.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.15.LayerNorm.bias\n",
      "esm.encoder.layer.15.LayerNorm.weight\n",
      "esm.encoder.layer.15.attention.LayerNorm.bias\n",
      "esm.encoder.layer.15.attention.LayerNorm.weight\n",
      "esm.encoder.layer.15.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.15.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.15.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.15.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.15.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.15.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.15.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.15.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.15.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.15.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.15.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.15.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.15.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.15.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.15.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.15.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.15.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.15.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.15.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.15.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.15.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.15.output.dense.base_layer.bias\n",
      "esm.encoder.layer.15.output.dense.base_layer.weight\n",
      "esm.encoder.layer.15.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.15.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.16.LayerNorm.bias\n",
      "esm.encoder.layer.16.LayerNorm.weight\n",
      "esm.encoder.layer.16.attention.LayerNorm.bias\n",
      "esm.encoder.layer.16.attention.LayerNorm.weight\n",
      "esm.encoder.layer.16.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.16.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.16.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.16.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.16.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.16.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.16.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.16.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.16.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.16.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.16.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.16.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.16.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.16.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.16.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.16.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.16.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.16.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.16.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.16.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.16.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.16.output.dense.base_layer.bias\n",
      "esm.encoder.layer.16.output.dense.base_layer.weight\n",
      "esm.encoder.layer.16.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.16.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.17.LayerNorm.bias\n",
      "esm.encoder.layer.17.LayerNorm.weight\n",
      "esm.encoder.layer.17.attention.LayerNorm.bias\n",
      "esm.encoder.layer.17.attention.LayerNorm.weight\n",
      "esm.encoder.layer.17.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.17.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.17.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.17.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.17.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.17.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.17.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.17.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.17.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.17.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.17.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.17.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.17.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.17.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.17.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.17.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.17.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.17.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.17.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.17.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.17.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.17.output.dense.base_layer.bias\n",
      "esm.encoder.layer.17.output.dense.base_layer.weight\n",
      "esm.encoder.layer.17.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.17.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.18.LayerNorm.bias\n",
      "esm.encoder.layer.18.LayerNorm.weight\n",
      "esm.encoder.layer.18.attention.LayerNorm.bias\n",
      "esm.encoder.layer.18.attention.LayerNorm.weight\n",
      "esm.encoder.layer.18.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.18.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.18.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.18.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.18.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.18.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.18.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.18.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.18.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.18.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.18.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.18.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.18.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.18.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.18.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.18.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.18.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.18.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.18.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.18.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.18.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.18.output.dense.base_layer.bias\n",
      "esm.encoder.layer.18.output.dense.base_layer.weight\n",
      "esm.encoder.layer.18.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.18.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.19.LayerNorm.bias\n",
      "esm.encoder.layer.19.LayerNorm.weight\n",
      "esm.encoder.layer.19.attention.LayerNorm.bias\n",
      "esm.encoder.layer.19.attention.LayerNorm.weight\n",
      "esm.encoder.layer.19.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.19.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.19.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.19.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.19.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.19.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.19.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.19.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.19.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.19.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.19.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.19.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.19.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.19.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.19.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.19.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.19.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.19.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.19.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.19.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.19.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.19.output.dense.base_layer.bias\n",
      "esm.encoder.layer.19.output.dense.base_layer.weight\n",
      "esm.encoder.layer.19.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.19.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.2.LayerNorm.bias\n",
      "esm.encoder.layer.2.LayerNorm.weight\n",
      "esm.encoder.layer.2.attention.LayerNorm.bias\n",
      "esm.encoder.layer.2.attention.LayerNorm.weight\n",
      "esm.encoder.layer.2.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.2.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.2.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.2.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.2.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.2.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.2.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.2.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.2.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.2.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.2.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.2.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.2.output.dense.base_layer.bias\n",
      "esm.encoder.layer.2.output.dense.base_layer.weight\n",
      "esm.encoder.layer.2.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.2.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.20.LayerNorm.bias\n",
      "esm.encoder.layer.20.LayerNorm.weight\n",
      "esm.encoder.layer.20.attention.LayerNorm.bias\n",
      "esm.encoder.layer.20.attention.LayerNorm.weight\n",
      "esm.encoder.layer.20.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.20.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.20.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.20.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.20.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.20.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.20.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.20.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.20.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.20.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.20.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.20.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.20.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.20.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.20.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.20.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.20.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.20.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.20.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.20.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.20.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.20.output.dense.base_layer.bias\n",
      "esm.encoder.layer.20.output.dense.base_layer.weight\n",
      "esm.encoder.layer.20.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.20.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.21.LayerNorm.bias\n",
      "esm.encoder.layer.21.LayerNorm.weight\n",
      "esm.encoder.layer.21.attention.LayerNorm.bias\n",
      "esm.encoder.layer.21.attention.LayerNorm.weight\n",
      "esm.encoder.layer.21.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.21.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.21.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.21.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.21.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.21.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.21.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.21.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.21.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.21.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.21.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.21.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.21.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.21.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.21.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.21.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.21.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.21.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.21.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.21.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.21.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.21.output.dense.base_layer.bias\n",
      "esm.encoder.layer.21.output.dense.base_layer.weight\n",
      "esm.encoder.layer.21.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.21.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.22.LayerNorm.bias\n",
      "esm.encoder.layer.22.LayerNorm.weight\n",
      "esm.encoder.layer.22.attention.LayerNorm.bias\n",
      "esm.encoder.layer.22.attention.LayerNorm.weight\n",
      "esm.encoder.layer.22.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.22.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.22.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.22.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.22.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.22.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.22.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.22.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.22.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.22.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.22.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.22.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.22.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.22.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.22.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.22.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.22.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.22.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.22.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.22.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.22.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.22.output.dense.base_layer.bias\n",
      "esm.encoder.layer.22.output.dense.base_layer.weight\n",
      "esm.encoder.layer.22.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.22.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.23.LayerNorm.bias\n",
      "esm.encoder.layer.23.LayerNorm.weight\n",
      "esm.encoder.layer.23.attention.LayerNorm.bias\n",
      "esm.encoder.layer.23.attention.LayerNorm.weight\n",
      "esm.encoder.layer.23.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.23.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.23.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.23.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.23.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.23.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.23.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.23.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.23.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.23.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.23.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.23.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.23.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.23.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.23.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.23.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.23.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.23.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.23.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.23.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.23.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.23.output.dense.base_layer.bias\n",
      "esm.encoder.layer.23.output.dense.base_layer.weight\n",
      "esm.encoder.layer.23.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.23.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.24.LayerNorm.bias\n",
      "esm.encoder.layer.24.LayerNorm.weight\n",
      "esm.encoder.layer.24.attention.LayerNorm.bias\n",
      "esm.encoder.layer.24.attention.LayerNorm.weight\n",
      "esm.encoder.layer.24.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.24.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.24.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.24.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.24.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.24.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.24.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.24.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.24.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.24.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.24.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.24.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.24.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.24.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.24.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.24.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.24.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.24.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.24.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.24.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.24.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.24.output.dense.base_layer.bias\n",
      "esm.encoder.layer.24.output.dense.base_layer.weight\n",
      "esm.encoder.layer.24.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.24.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.25.LayerNorm.bias\n",
      "esm.encoder.layer.25.LayerNorm.weight\n",
      "esm.encoder.layer.25.attention.LayerNorm.bias\n",
      "esm.encoder.layer.25.attention.LayerNorm.weight\n",
      "esm.encoder.layer.25.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.25.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.25.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.25.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.25.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.25.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.25.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.25.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.25.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.25.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.25.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.25.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.25.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.25.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.25.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.25.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.25.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.25.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.25.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.25.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.25.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.25.output.dense.base_layer.bias\n",
      "esm.encoder.layer.25.output.dense.base_layer.weight\n",
      "esm.encoder.layer.25.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.25.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.26.LayerNorm.bias\n",
      "esm.encoder.layer.26.LayerNorm.weight\n",
      "esm.encoder.layer.26.attention.LayerNorm.bias\n",
      "esm.encoder.layer.26.attention.LayerNorm.weight\n",
      "esm.encoder.layer.26.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.26.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.26.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.26.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.26.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.26.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.26.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.26.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.26.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.26.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.26.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.26.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.26.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.26.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.26.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.26.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.26.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.26.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.26.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.26.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.26.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.26.output.dense.base_layer.bias\n",
      "esm.encoder.layer.26.output.dense.base_layer.weight\n",
      "esm.encoder.layer.26.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.26.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.27.LayerNorm.bias\n",
      "esm.encoder.layer.27.LayerNorm.weight\n",
      "esm.encoder.layer.27.attention.LayerNorm.bias\n",
      "esm.encoder.layer.27.attention.LayerNorm.weight\n",
      "esm.encoder.layer.27.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.27.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.27.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.27.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.27.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.27.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.27.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.27.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.27.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.27.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.27.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.27.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.27.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.27.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.27.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.27.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.27.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.27.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.27.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.27.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.27.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.27.output.dense.base_layer.bias\n",
      "esm.encoder.layer.27.output.dense.base_layer.weight\n",
      "esm.encoder.layer.27.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.27.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.28.LayerNorm.bias\n",
      "esm.encoder.layer.28.LayerNorm.weight\n",
      "esm.encoder.layer.28.attention.LayerNorm.bias\n",
      "esm.encoder.layer.28.attention.LayerNorm.weight\n",
      "esm.encoder.layer.28.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.28.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.28.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.28.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.28.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.28.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.28.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.28.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.28.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.28.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.28.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.28.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.28.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.28.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.28.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.28.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.28.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.28.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.28.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.28.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.28.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.28.output.dense.base_layer.bias\n",
      "esm.encoder.layer.28.output.dense.base_layer.weight\n",
      "esm.encoder.layer.28.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.28.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.29.LayerNorm.bias\n",
      "esm.encoder.layer.29.LayerNorm.weight\n",
      "esm.encoder.layer.29.attention.LayerNorm.bias\n",
      "esm.encoder.layer.29.attention.LayerNorm.weight\n",
      "esm.encoder.layer.29.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.29.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.29.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.29.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.29.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.29.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.29.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.29.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.29.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.29.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.29.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.29.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.29.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.29.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.29.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.29.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.29.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.29.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.29.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.29.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.29.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.29.output.dense.base_layer.bias\n",
      "esm.encoder.layer.29.output.dense.base_layer.weight\n",
      "esm.encoder.layer.29.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.29.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.3.LayerNorm.bias\n",
      "esm.encoder.layer.3.LayerNorm.weight\n",
      "esm.encoder.layer.3.attention.LayerNorm.bias\n",
      "esm.encoder.layer.3.attention.LayerNorm.weight\n",
      "esm.encoder.layer.3.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.3.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.3.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.3.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.3.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.3.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.3.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.3.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.3.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.3.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.3.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.3.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.3.output.dense.base_layer.bias\n",
      "esm.encoder.layer.3.output.dense.base_layer.weight\n",
      "esm.encoder.layer.3.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.3.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.4.LayerNorm.bias\n",
      "esm.encoder.layer.4.LayerNorm.weight\n",
      "esm.encoder.layer.4.attention.LayerNorm.bias\n",
      "esm.encoder.layer.4.attention.LayerNorm.weight\n",
      "esm.encoder.layer.4.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.4.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.4.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.4.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.4.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.4.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.4.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.4.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.4.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.4.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.4.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.4.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.4.output.dense.base_layer.bias\n",
      "esm.encoder.layer.4.output.dense.base_layer.weight\n",
      "esm.encoder.layer.4.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.4.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.5.LayerNorm.bias\n",
      "esm.encoder.layer.5.LayerNorm.weight\n",
      "esm.encoder.layer.5.attention.LayerNorm.bias\n",
      "esm.encoder.layer.5.attention.LayerNorm.weight\n",
      "esm.encoder.layer.5.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.5.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.5.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.5.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.5.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.5.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.5.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.5.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.5.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.5.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.5.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.5.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.5.output.dense.base_layer.bias\n",
      "esm.encoder.layer.5.output.dense.base_layer.weight\n",
      "esm.encoder.layer.5.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.5.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.6.LayerNorm.bias\n",
      "esm.encoder.layer.6.LayerNorm.weight\n",
      "esm.encoder.layer.6.attention.LayerNorm.bias\n",
      "esm.encoder.layer.6.attention.LayerNorm.weight\n",
      "esm.encoder.layer.6.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.6.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.6.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.6.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.6.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.6.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.6.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.6.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.6.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.6.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.6.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.6.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.6.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.6.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.6.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.6.output.dense.base_layer.bias\n",
      "esm.encoder.layer.6.output.dense.base_layer.weight\n",
      "esm.encoder.layer.6.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.6.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.7.LayerNorm.bias\n",
      "esm.encoder.layer.7.LayerNorm.weight\n",
      "esm.encoder.layer.7.attention.LayerNorm.bias\n",
      "esm.encoder.layer.7.attention.LayerNorm.weight\n",
      "esm.encoder.layer.7.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.7.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.7.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.7.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.7.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.7.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.7.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.7.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.7.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.7.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.7.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.7.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.7.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.7.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.7.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.7.output.dense.base_layer.bias\n",
      "esm.encoder.layer.7.output.dense.base_layer.weight\n",
      "esm.encoder.layer.7.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.7.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.8.LayerNorm.bias\n",
      "esm.encoder.layer.8.LayerNorm.weight\n",
      "esm.encoder.layer.8.attention.LayerNorm.bias\n",
      "esm.encoder.layer.8.attention.LayerNorm.weight\n",
      "esm.encoder.layer.8.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.8.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.8.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.8.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.8.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.8.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.8.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.8.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.8.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.8.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.8.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.8.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.8.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.8.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.8.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.8.output.dense.base_layer.bias\n",
      "esm.encoder.layer.8.output.dense.base_layer.weight\n",
      "esm.encoder.layer.8.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.8.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.9.LayerNorm.bias\n",
      "esm.encoder.layer.9.LayerNorm.weight\n",
      "esm.encoder.layer.9.attention.LayerNorm.bias\n",
      "esm.encoder.layer.9.attention.LayerNorm.weight\n",
      "esm.encoder.layer.9.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.9.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.9.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.9.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.9.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.9.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.9.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.9.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.9.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.9.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.9.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.9.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.9.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.9.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.9.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.9.output.dense.base_layer.bias\n",
      "esm.encoder.layer.9.output.dense.base_layer.weight\n",
      "esm.encoder.layer.9.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.9.output.dense.lora_B.default.weight\n",
      "lm_head.decoder.bias\n",
      "lm_head.decoder.weight\n",
      "lm_head.dense.base_layer.bias\n",
      "lm_head.dense.base_layer.weight\n",
      "lm_head.dense.lora_A.default.weight\n",
      "lm_head.dense.lora_B.default.weight\n",
      "lm_head.layer_norm.bias\n",
      "lm_head.layer_norm.weight\n",
      "pooler.dense.base_layer.bias\n",
      "pooler.dense.base_layer.weight\n",
      "pooler.dense.lora_A.default.weight\n",
      "pooler.dense.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "for key in state_dict.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained('Synthyra/ESMplusplus_small', trust_remote_code=True)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "sequences = ['MPRTEIN', 'MSEQWENCE']\n",
    "tokenized = tokenizer(sequences, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 0, 32, 31]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affinity', 'binding']\n"
     ]
    }
   ],
   "source": [
    "subtypes = ['affinity', 'binding']\n",
    "\n",
    "f'{[subtype for subtype in subtypes]}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

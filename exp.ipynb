{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seqs', 'labels'],\n",
       "        num_rows: 10792\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['seqs', 'labels'],\n",
       "        num_rows: 626\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seqs', 'labels'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset('GleghornLab/SS8')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B', 'C', 'D', 'E', 'G', 'H', 'I', 'S', 'T'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for label in data['train']['labels']:\n",
    "    vocab.update(label)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'].to_pandas().to_csv('ss8_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from huggingface_hub import hf_hub_download\n",
    "from models.modeling_esm_diff import ESM_Diff_Binders, ESMDiffConfig\n",
    "from models.utils import wrap_lora\n",
    "\n",
    "MODEL_PATH = 'lhallee/esm_diff_bind_150'\n",
    "base_path = 'GleghornLab/esm_diff_150'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 856 parameters\n",
      "Missing 0 parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoraModel(\n",
       "  (model): ESM_Diff_Binders(\n",
       "    (esm): FAST_ESM_ENCODER(\n",
       "      (embeddings): EsmEmbeddings(\n",
       "        (word_embeddings): Embedding(33, 640, padding_idx=1)\n",
       "      )\n",
       "      (encoder): EsmEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-29): 30 x EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (key): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (value): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): lora.Linear(\n",
       "                (base_layer): Linear(in_features=640, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (contact_head): EsmContactPredictionHead(\n",
       "        (regression): Linear(in_features=600, out_features=1, bias=True)\n",
       "        (activation): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (pooler): EsmPooler(\n",
       "      (dense): lora.Linear(\n",
       "        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (lm_head): LMHead(\n",
       "      (dense): lora.Linear(\n",
       "        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=640, out_features=33, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (ce_loss): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "local_weight_file = hf_hub_download(\n",
    "    repo_id=MODEL_PATH,\n",
    "    filename='model.safetensors',\n",
    "    repo_type='model',\n",
    ")\n",
    "\n",
    "config = ESMDiffConfig.from_pretrained(MODEL_PATH)\n",
    "model = ESM_Diff_Binders(config=config)\n",
    "model = wrap_lora(model, r=config.lora_r, lora_alpha=config.lora_alpha, lora_dropout=config.lora_dropout)\n",
    "state_dict = load_file(local_weight_file)\n",
    "\n",
    "# Track which parameters were loaded\n",
    "loaded_params = set()\n",
    "missing_params = set()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    found = False\n",
    "    for key in state_dict.keys():\n",
    "        if key in name:\n",
    "            param.data = state_dict[key]\n",
    "            loaded_params.add(name)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        missing_params.add(name)\n",
    "\n",
    "# Verify all weights were loaded correctly\n",
    "print(f\"Loaded {len(loaded_params)} parameters\")\n",
    "print(f\"Missing {len(missing_params)} parameters\")\n",
    "if missing_params:\n",
    "    print(\"Missing parameters:\")\n",
    "    for param in sorted(missing_params):\n",
    "        print(f\"  - {param}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.esm.embeddings.word_embeddings.weight\n",
      "model.esm.encoder.layer.0.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.0.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.0.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.0.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.0.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.0.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.0.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.0.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.0.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.0.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.0.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.0.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.0.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.0.LayerNorm.weight\n",
      "model.esm.encoder.layer.0.LayerNorm.bias\n",
      "model.esm.encoder.layer.1.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.1.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.1.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.1.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.1.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.1.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.1.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.1.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.1.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.1.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.1.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.1.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.1.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.1.LayerNorm.weight\n",
      "model.esm.encoder.layer.1.LayerNorm.bias\n",
      "model.esm.encoder.layer.2.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.2.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.2.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.2.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.2.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.2.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.2.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.2.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.2.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.2.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.2.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.2.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.2.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.2.LayerNorm.weight\n",
      "model.esm.encoder.layer.2.LayerNorm.bias\n",
      "model.esm.encoder.layer.3.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.3.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.3.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.3.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.3.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.3.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.3.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.3.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.3.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.3.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.3.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.3.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.3.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.3.LayerNorm.weight\n",
      "model.esm.encoder.layer.3.LayerNorm.bias\n",
      "model.esm.encoder.layer.4.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.4.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.4.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.4.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.4.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.4.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.4.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.4.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.4.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.4.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.4.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.4.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.4.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.4.LayerNorm.weight\n",
      "model.esm.encoder.layer.4.LayerNorm.bias\n",
      "model.esm.encoder.layer.5.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.5.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.5.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.5.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.5.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.5.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.5.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.5.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.5.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.5.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.5.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.5.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.5.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.5.LayerNorm.weight\n",
      "model.esm.encoder.layer.5.LayerNorm.bias\n",
      "model.esm.encoder.layer.6.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.6.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.6.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.6.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.6.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.6.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.6.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.6.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.6.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.6.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.6.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.6.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.6.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.6.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.6.LayerNorm.weight\n",
      "model.esm.encoder.layer.6.LayerNorm.bias\n",
      "model.esm.encoder.layer.7.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.7.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.7.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.7.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.7.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.7.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.7.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.7.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.7.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.7.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.7.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.7.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.7.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.7.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.7.LayerNorm.weight\n",
      "model.esm.encoder.layer.7.LayerNorm.bias\n",
      "model.esm.encoder.layer.8.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.8.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.8.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.8.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.8.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.8.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.8.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.8.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.8.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.8.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.8.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.8.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.8.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.8.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.8.LayerNorm.weight\n",
      "model.esm.encoder.layer.8.LayerNorm.bias\n",
      "model.esm.encoder.layer.9.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.9.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.9.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.9.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.9.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.9.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.9.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.9.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.9.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.9.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.9.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.9.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.9.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.9.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.9.LayerNorm.weight\n",
      "model.esm.encoder.layer.9.LayerNorm.bias\n",
      "model.esm.encoder.layer.10.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.10.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.10.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.10.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.10.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.10.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.10.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.10.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.10.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.10.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.10.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.10.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.10.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.10.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.10.LayerNorm.weight\n",
      "model.esm.encoder.layer.10.LayerNorm.bias\n",
      "model.esm.encoder.layer.11.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.11.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.11.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.11.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.11.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.11.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.11.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.11.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.11.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.11.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.11.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.11.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.11.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.11.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.11.LayerNorm.weight\n",
      "model.esm.encoder.layer.11.LayerNorm.bias\n",
      "model.esm.encoder.layer.12.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.12.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.12.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.12.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.12.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.12.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.12.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.12.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.12.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.12.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.12.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.12.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.12.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.12.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.12.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.12.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.12.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.12.LayerNorm.weight\n",
      "model.esm.encoder.layer.12.LayerNorm.bias\n",
      "model.esm.encoder.layer.13.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.13.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.13.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.13.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.13.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.13.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.13.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.13.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.13.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.13.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.13.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.13.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.13.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.13.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.13.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.13.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.13.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.13.LayerNorm.weight\n",
      "model.esm.encoder.layer.13.LayerNorm.bias\n",
      "model.esm.encoder.layer.14.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.14.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.14.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.14.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.14.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.14.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.14.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.14.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.14.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.14.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.14.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.14.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.14.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.14.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.14.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.14.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.14.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.14.LayerNorm.weight\n",
      "model.esm.encoder.layer.14.LayerNorm.bias\n",
      "model.esm.encoder.layer.15.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.15.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.15.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.15.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.15.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.15.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.15.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.15.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.15.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.15.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.15.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.15.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.15.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.15.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.15.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.15.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.15.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.15.LayerNorm.weight\n",
      "model.esm.encoder.layer.15.LayerNorm.bias\n",
      "model.esm.encoder.layer.16.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.16.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.16.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.16.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.16.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.16.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.16.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.16.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.16.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.16.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.16.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.16.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.16.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.16.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.16.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.16.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.16.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.16.LayerNorm.weight\n",
      "model.esm.encoder.layer.16.LayerNorm.bias\n",
      "model.esm.encoder.layer.17.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.17.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.17.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.17.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.17.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.17.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.17.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.17.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.17.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.17.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.17.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.17.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.17.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.17.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.17.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.17.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.17.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.17.LayerNorm.weight\n",
      "model.esm.encoder.layer.17.LayerNorm.bias\n",
      "model.esm.encoder.layer.18.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.18.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.18.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.18.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.18.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.18.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.18.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.18.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.18.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.18.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.18.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.18.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.18.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.18.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.18.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.18.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.18.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.18.LayerNorm.weight\n",
      "model.esm.encoder.layer.18.LayerNorm.bias\n",
      "model.esm.encoder.layer.19.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.19.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.19.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.19.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.19.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.19.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.19.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.19.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.19.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.19.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.19.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.19.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.19.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.19.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.19.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.19.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.19.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.19.LayerNorm.weight\n",
      "model.esm.encoder.layer.19.LayerNorm.bias\n",
      "model.esm.encoder.layer.20.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.20.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.20.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.20.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.20.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.20.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.20.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.20.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.20.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.20.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.20.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.20.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.20.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.20.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.20.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.20.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.20.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.20.LayerNorm.weight\n",
      "model.esm.encoder.layer.20.LayerNorm.bias\n",
      "model.esm.encoder.layer.21.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.21.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.21.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.21.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.21.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.21.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.21.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.21.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.21.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.21.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.21.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.21.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.21.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.21.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.21.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.21.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.21.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.21.LayerNorm.weight\n",
      "model.esm.encoder.layer.21.LayerNorm.bias\n",
      "model.esm.encoder.layer.22.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.22.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.22.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.22.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.22.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.22.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.22.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.22.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.22.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.22.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.22.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.22.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.22.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.22.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.22.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.22.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.22.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.22.LayerNorm.weight\n",
      "model.esm.encoder.layer.22.LayerNorm.bias\n",
      "model.esm.encoder.layer.23.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.23.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.23.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.23.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.23.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.23.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.23.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.23.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.23.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.23.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.23.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.23.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.23.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.23.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.23.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.23.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.23.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.23.LayerNorm.weight\n",
      "model.esm.encoder.layer.23.LayerNorm.bias\n",
      "model.esm.encoder.layer.24.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.24.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.24.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.24.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.24.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.24.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.24.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.24.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.24.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.24.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.24.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.24.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.24.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.24.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.24.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.24.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.24.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.24.LayerNorm.weight\n",
      "model.esm.encoder.layer.24.LayerNorm.bias\n",
      "model.esm.encoder.layer.25.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.25.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.25.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.25.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.25.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.25.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.25.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.25.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.25.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.25.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.25.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.25.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.25.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.25.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.25.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.25.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.25.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.25.LayerNorm.weight\n",
      "model.esm.encoder.layer.25.LayerNorm.bias\n",
      "model.esm.encoder.layer.26.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.26.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.26.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.26.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.26.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.26.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.26.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.26.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.26.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.26.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.26.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.26.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.26.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.26.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.26.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.26.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.26.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.26.LayerNorm.weight\n",
      "model.esm.encoder.layer.26.LayerNorm.bias\n",
      "model.esm.encoder.layer.27.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.27.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.27.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.27.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.27.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.27.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.27.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.27.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.27.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.27.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.27.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.27.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.27.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.27.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.27.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.27.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.27.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.27.LayerNorm.weight\n",
      "model.esm.encoder.layer.27.LayerNorm.bias\n",
      "model.esm.encoder.layer.28.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.28.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.28.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.28.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.28.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.28.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.28.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.28.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.28.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.28.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.28.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.28.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.28.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.28.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.28.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.28.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.28.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.28.LayerNorm.weight\n",
      "model.esm.encoder.layer.28.LayerNorm.bias\n",
      "model.esm.encoder.layer.29.attention.self.query.base_layer.weight\n",
      "model.esm.encoder.layer.29.attention.self.query.base_layer.bias\n",
      "model.esm.encoder.layer.29.attention.self.query.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.query.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.key.base_layer.weight\n",
      "model.esm.encoder.layer.29.attention.self.key.base_layer.bias\n",
      "model.esm.encoder.layer.29.attention.self.key.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.key.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.value.base_layer.weight\n",
      "model.esm.encoder.layer.29.attention.self.value.base_layer.bias\n",
      "model.esm.encoder.layer.29.attention.self.value.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.value.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.attention.self.rotary_embeddings.inv_freq\n",
      "model.esm.encoder.layer.29.attention.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.29.attention.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.29.attention.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.attention.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.attention.LayerNorm.weight\n",
      "model.esm.encoder.layer.29.attention.LayerNorm.bias\n",
      "model.esm.encoder.layer.29.intermediate.dense.base_layer.weight\n",
      "model.esm.encoder.layer.29.intermediate.dense.base_layer.bias\n",
      "model.esm.encoder.layer.29.intermediate.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.intermediate.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.output.dense.base_layer.weight\n",
      "model.esm.encoder.layer.29.output.dense.base_layer.bias\n",
      "model.esm.encoder.layer.29.output.dense.lora_A.default.weight\n",
      "model.esm.encoder.layer.29.output.dense.lora_B.default.weight\n",
      "model.esm.encoder.layer.29.LayerNorm.weight\n",
      "model.esm.encoder.layer.29.LayerNorm.bias\n",
      "model.esm.encoder.emb_layer_norm_after.weight\n",
      "model.esm.encoder.emb_layer_norm_after.bias\n",
      "model.esm.contact_head.regression.weight\n",
      "model.esm.contact_head.regression.bias\n",
      "model.pooler.dense.base_layer.weight\n",
      "model.pooler.dense.base_layer.bias\n",
      "model.pooler.dense.lora_A.default.weight\n",
      "model.pooler.dense.lora_B.default.weight\n",
      "model.lm_head.dense.base_layer.weight\n",
      "model.lm_head.dense.base_layer.bias\n",
      "model.lm_head.dense.lora_A.default.weight\n",
      "model.lm_head.dense.lora_B.default.weight\n",
      "model.lm_head.layer_norm.weight\n",
      "model.lm_head.layer_norm.bias\n",
      "model.lm_head.decoder.weight\n",
      "model.lm_head.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm.contact_head.regression.bias\n",
      "esm.contact_head.regression.weight\n",
      "esm.embeddings.word_embeddings.weight\n",
      "esm.encoder.emb_layer_norm_after.bias\n",
      "esm.encoder.emb_layer_norm_after.weight\n",
      "esm.encoder.layer.0.LayerNorm.bias\n",
      "esm.encoder.layer.0.LayerNorm.weight\n",
      "esm.encoder.layer.0.attention.LayerNorm.bias\n",
      "esm.encoder.layer.0.attention.LayerNorm.weight\n",
      "esm.encoder.layer.0.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.0.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.0.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.0.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.0.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.0.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.0.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.0.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.0.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.0.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.0.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.0.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.0.output.dense.base_layer.bias\n",
      "esm.encoder.layer.0.output.dense.base_layer.weight\n",
      "esm.encoder.layer.0.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.0.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.1.LayerNorm.bias\n",
      "esm.encoder.layer.1.LayerNorm.weight\n",
      "esm.encoder.layer.1.attention.LayerNorm.bias\n",
      "esm.encoder.layer.1.attention.LayerNorm.weight\n",
      "esm.encoder.layer.1.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.1.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.1.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.1.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.1.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.1.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.1.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.1.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.1.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.1.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.1.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.1.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.1.output.dense.base_layer.bias\n",
      "esm.encoder.layer.1.output.dense.base_layer.weight\n",
      "esm.encoder.layer.1.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.1.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.10.LayerNorm.bias\n",
      "esm.encoder.layer.10.LayerNorm.weight\n",
      "esm.encoder.layer.10.attention.LayerNorm.bias\n",
      "esm.encoder.layer.10.attention.LayerNorm.weight\n",
      "esm.encoder.layer.10.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.10.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.10.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.10.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.10.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.10.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.10.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.10.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.10.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.10.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.10.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.10.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.10.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.10.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.10.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.10.output.dense.base_layer.bias\n",
      "esm.encoder.layer.10.output.dense.base_layer.weight\n",
      "esm.encoder.layer.10.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.10.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.11.LayerNorm.bias\n",
      "esm.encoder.layer.11.LayerNorm.weight\n",
      "esm.encoder.layer.11.attention.LayerNorm.bias\n",
      "esm.encoder.layer.11.attention.LayerNorm.weight\n",
      "esm.encoder.layer.11.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.11.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.11.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.11.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.11.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.11.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.11.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.11.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.11.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.11.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.11.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.11.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.11.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.11.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.11.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.11.output.dense.base_layer.bias\n",
      "esm.encoder.layer.11.output.dense.base_layer.weight\n",
      "esm.encoder.layer.11.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.11.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.12.LayerNorm.bias\n",
      "esm.encoder.layer.12.LayerNorm.weight\n",
      "esm.encoder.layer.12.attention.LayerNorm.bias\n",
      "esm.encoder.layer.12.attention.LayerNorm.weight\n",
      "esm.encoder.layer.12.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.12.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.12.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.12.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.12.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.12.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.12.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.12.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.12.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.12.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.12.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.12.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.12.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.12.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.12.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.12.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.12.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.12.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.12.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.12.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.12.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.12.output.dense.base_layer.bias\n",
      "esm.encoder.layer.12.output.dense.base_layer.weight\n",
      "esm.encoder.layer.12.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.12.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.13.LayerNorm.bias\n",
      "esm.encoder.layer.13.LayerNorm.weight\n",
      "esm.encoder.layer.13.attention.LayerNorm.bias\n",
      "esm.encoder.layer.13.attention.LayerNorm.weight\n",
      "esm.encoder.layer.13.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.13.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.13.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.13.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.13.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.13.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.13.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.13.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.13.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.13.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.13.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.13.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.13.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.13.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.13.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.13.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.13.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.13.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.13.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.13.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.13.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.13.output.dense.base_layer.bias\n",
      "esm.encoder.layer.13.output.dense.base_layer.weight\n",
      "esm.encoder.layer.13.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.13.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.14.LayerNorm.bias\n",
      "esm.encoder.layer.14.LayerNorm.weight\n",
      "esm.encoder.layer.14.attention.LayerNorm.bias\n",
      "esm.encoder.layer.14.attention.LayerNorm.weight\n",
      "esm.encoder.layer.14.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.14.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.14.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.14.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.14.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.14.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.14.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.14.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.14.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.14.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.14.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.14.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.14.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.14.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.14.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.14.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.14.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.14.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.14.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.14.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.14.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.14.output.dense.base_layer.bias\n",
      "esm.encoder.layer.14.output.dense.base_layer.weight\n",
      "esm.encoder.layer.14.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.14.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.15.LayerNorm.bias\n",
      "esm.encoder.layer.15.LayerNorm.weight\n",
      "esm.encoder.layer.15.attention.LayerNorm.bias\n",
      "esm.encoder.layer.15.attention.LayerNorm.weight\n",
      "esm.encoder.layer.15.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.15.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.15.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.15.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.15.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.15.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.15.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.15.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.15.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.15.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.15.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.15.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.15.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.15.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.15.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.15.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.15.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.15.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.15.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.15.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.15.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.15.output.dense.base_layer.bias\n",
      "esm.encoder.layer.15.output.dense.base_layer.weight\n",
      "esm.encoder.layer.15.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.15.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.16.LayerNorm.bias\n",
      "esm.encoder.layer.16.LayerNorm.weight\n",
      "esm.encoder.layer.16.attention.LayerNorm.bias\n",
      "esm.encoder.layer.16.attention.LayerNorm.weight\n",
      "esm.encoder.layer.16.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.16.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.16.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.16.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.16.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.16.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.16.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.16.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.16.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.16.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.16.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.16.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.16.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.16.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.16.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.16.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.16.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.16.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.16.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.16.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.16.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.16.output.dense.base_layer.bias\n",
      "esm.encoder.layer.16.output.dense.base_layer.weight\n",
      "esm.encoder.layer.16.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.16.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.17.LayerNorm.bias\n",
      "esm.encoder.layer.17.LayerNorm.weight\n",
      "esm.encoder.layer.17.attention.LayerNorm.bias\n",
      "esm.encoder.layer.17.attention.LayerNorm.weight\n",
      "esm.encoder.layer.17.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.17.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.17.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.17.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.17.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.17.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.17.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.17.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.17.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.17.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.17.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.17.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.17.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.17.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.17.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.17.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.17.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.17.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.17.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.17.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.17.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.17.output.dense.base_layer.bias\n",
      "esm.encoder.layer.17.output.dense.base_layer.weight\n",
      "esm.encoder.layer.17.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.17.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.18.LayerNorm.bias\n",
      "esm.encoder.layer.18.LayerNorm.weight\n",
      "esm.encoder.layer.18.attention.LayerNorm.bias\n",
      "esm.encoder.layer.18.attention.LayerNorm.weight\n",
      "esm.encoder.layer.18.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.18.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.18.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.18.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.18.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.18.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.18.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.18.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.18.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.18.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.18.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.18.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.18.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.18.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.18.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.18.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.18.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.18.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.18.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.18.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.18.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.18.output.dense.base_layer.bias\n",
      "esm.encoder.layer.18.output.dense.base_layer.weight\n",
      "esm.encoder.layer.18.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.18.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.19.LayerNorm.bias\n",
      "esm.encoder.layer.19.LayerNorm.weight\n",
      "esm.encoder.layer.19.attention.LayerNorm.bias\n",
      "esm.encoder.layer.19.attention.LayerNorm.weight\n",
      "esm.encoder.layer.19.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.19.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.19.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.19.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.19.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.19.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.19.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.19.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.19.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.19.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.19.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.19.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.19.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.19.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.19.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.19.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.19.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.19.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.19.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.19.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.19.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.19.output.dense.base_layer.bias\n",
      "esm.encoder.layer.19.output.dense.base_layer.weight\n",
      "esm.encoder.layer.19.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.19.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.2.LayerNorm.bias\n",
      "esm.encoder.layer.2.LayerNorm.weight\n",
      "esm.encoder.layer.2.attention.LayerNorm.bias\n",
      "esm.encoder.layer.2.attention.LayerNorm.weight\n",
      "esm.encoder.layer.2.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.2.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.2.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.2.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.2.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.2.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.2.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.2.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.2.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.2.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.2.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.2.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.2.output.dense.base_layer.bias\n",
      "esm.encoder.layer.2.output.dense.base_layer.weight\n",
      "esm.encoder.layer.2.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.2.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.20.LayerNorm.bias\n",
      "esm.encoder.layer.20.LayerNorm.weight\n",
      "esm.encoder.layer.20.attention.LayerNorm.bias\n",
      "esm.encoder.layer.20.attention.LayerNorm.weight\n",
      "esm.encoder.layer.20.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.20.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.20.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.20.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.20.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.20.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.20.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.20.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.20.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.20.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.20.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.20.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.20.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.20.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.20.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.20.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.20.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.20.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.20.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.20.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.20.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.20.output.dense.base_layer.bias\n",
      "esm.encoder.layer.20.output.dense.base_layer.weight\n",
      "esm.encoder.layer.20.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.20.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.21.LayerNorm.bias\n",
      "esm.encoder.layer.21.LayerNorm.weight\n",
      "esm.encoder.layer.21.attention.LayerNorm.bias\n",
      "esm.encoder.layer.21.attention.LayerNorm.weight\n",
      "esm.encoder.layer.21.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.21.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.21.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.21.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.21.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.21.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.21.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.21.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.21.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.21.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.21.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.21.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.21.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.21.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.21.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.21.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.21.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.21.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.21.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.21.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.21.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.21.output.dense.base_layer.bias\n",
      "esm.encoder.layer.21.output.dense.base_layer.weight\n",
      "esm.encoder.layer.21.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.21.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.22.LayerNorm.bias\n",
      "esm.encoder.layer.22.LayerNorm.weight\n",
      "esm.encoder.layer.22.attention.LayerNorm.bias\n",
      "esm.encoder.layer.22.attention.LayerNorm.weight\n",
      "esm.encoder.layer.22.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.22.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.22.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.22.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.22.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.22.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.22.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.22.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.22.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.22.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.22.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.22.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.22.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.22.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.22.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.22.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.22.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.22.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.22.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.22.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.22.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.22.output.dense.base_layer.bias\n",
      "esm.encoder.layer.22.output.dense.base_layer.weight\n",
      "esm.encoder.layer.22.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.22.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.23.LayerNorm.bias\n",
      "esm.encoder.layer.23.LayerNorm.weight\n",
      "esm.encoder.layer.23.attention.LayerNorm.bias\n",
      "esm.encoder.layer.23.attention.LayerNorm.weight\n",
      "esm.encoder.layer.23.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.23.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.23.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.23.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.23.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.23.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.23.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.23.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.23.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.23.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.23.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.23.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.23.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.23.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.23.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.23.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.23.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.23.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.23.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.23.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.23.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.23.output.dense.base_layer.bias\n",
      "esm.encoder.layer.23.output.dense.base_layer.weight\n",
      "esm.encoder.layer.23.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.23.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.24.LayerNorm.bias\n",
      "esm.encoder.layer.24.LayerNorm.weight\n",
      "esm.encoder.layer.24.attention.LayerNorm.bias\n",
      "esm.encoder.layer.24.attention.LayerNorm.weight\n",
      "esm.encoder.layer.24.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.24.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.24.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.24.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.24.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.24.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.24.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.24.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.24.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.24.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.24.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.24.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.24.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.24.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.24.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.24.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.24.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.24.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.24.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.24.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.24.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.24.output.dense.base_layer.bias\n",
      "esm.encoder.layer.24.output.dense.base_layer.weight\n",
      "esm.encoder.layer.24.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.24.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.25.LayerNorm.bias\n",
      "esm.encoder.layer.25.LayerNorm.weight\n",
      "esm.encoder.layer.25.attention.LayerNorm.bias\n",
      "esm.encoder.layer.25.attention.LayerNorm.weight\n",
      "esm.encoder.layer.25.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.25.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.25.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.25.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.25.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.25.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.25.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.25.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.25.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.25.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.25.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.25.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.25.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.25.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.25.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.25.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.25.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.25.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.25.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.25.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.25.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.25.output.dense.base_layer.bias\n",
      "esm.encoder.layer.25.output.dense.base_layer.weight\n",
      "esm.encoder.layer.25.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.25.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.26.LayerNorm.bias\n",
      "esm.encoder.layer.26.LayerNorm.weight\n",
      "esm.encoder.layer.26.attention.LayerNorm.bias\n",
      "esm.encoder.layer.26.attention.LayerNorm.weight\n",
      "esm.encoder.layer.26.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.26.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.26.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.26.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.26.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.26.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.26.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.26.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.26.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.26.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.26.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.26.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.26.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.26.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.26.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.26.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.26.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.26.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.26.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.26.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.26.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.26.output.dense.base_layer.bias\n",
      "esm.encoder.layer.26.output.dense.base_layer.weight\n",
      "esm.encoder.layer.26.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.26.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.27.LayerNorm.bias\n",
      "esm.encoder.layer.27.LayerNorm.weight\n",
      "esm.encoder.layer.27.attention.LayerNorm.bias\n",
      "esm.encoder.layer.27.attention.LayerNorm.weight\n",
      "esm.encoder.layer.27.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.27.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.27.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.27.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.27.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.27.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.27.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.27.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.27.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.27.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.27.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.27.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.27.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.27.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.27.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.27.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.27.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.27.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.27.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.27.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.27.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.27.output.dense.base_layer.bias\n",
      "esm.encoder.layer.27.output.dense.base_layer.weight\n",
      "esm.encoder.layer.27.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.27.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.28.LayerNorm.bias\n",
      "esm.encoder.layer.28.LayerNorm.weight\n",
      "esm.encoder.layer.28.attention.LayerNorm.bias\n",
      "esm.encoder.layer.28.attention.LayerNorm.weight\n",
      "esm.encoder.layer.28.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.28.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.28.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.28.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.28.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.28.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.28.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.28.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.28.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.28.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.28.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.28.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.28.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.28.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.28.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.28.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.28.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.28.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.28.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.28.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.28.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.28.output.dense.base_layer.bias\n",
      "esm.encoder.layer.28.output.dense.base_layer.weight\n",
      "esm.encoder.layer.28.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.28.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.29.LayerNorm.bias\n",
      "esm.encoder.layer.29.LayerNorm.weight\n",
      "esm.encoder.layer.29.attention.LayerNorm.bias\n",
      "esm.encoder.layer.29.attention.LayerNorm.weight\n",
      "esm.encoder.layer.29.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.29.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.29.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.29.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.29.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.29.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.29.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.29.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.29.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.29.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.29.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.29.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.29.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.29.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.29.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.29.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.29.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.29.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.29.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.29.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.29.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.29.output.dense.base_layer.bias\n",
      "esm.encoder.layer.29.output.dense.base_layer.weight\n",
      "esm.encoder.layer.29.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.29.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.3.LayerNorm.bias\n",
      "esm.encoder.layer.3.LayerNorm.weight\n",
      "esm.encoder.layer.3.attention.LayerNorm.bias\n",
      "esm.encoder.layer.3.attention.LayerNorm.weight\n",
      "esm.encoder.layer.3.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.3.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.3.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.3.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.3.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.3.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.3.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.3.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.3.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.3.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.3.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.3.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.3.output.dense.base_layer.bias\n",
      "esm.encoder.layer.3.output.dense.base_layer.weight\n",
      "esm.encoder.layer.3.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.3.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.4.LayerNorm.bias\n",
      "esm.encoder.layer.4.LayerNorm.weight\n",
      "esm.encoder.layer.4.attention.LayerNorm.bias\n",
      "esm.encoder.layer.4.attention.LayerNorm.weight\n",
      "esm.encoder.layer.4.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.4.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.4.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.4.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.4.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.4.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.4.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.4.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.4.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.4.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.4.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.4.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.4.output.dense.base_layer.bias\n",
      "esm.encoder.layer.4.output.dense.base_layer.weight\n",
      "esm.encoder.layer.4.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.4.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.5.LayerNorm.bias\n",
      "esm.encoder.layer.5.LayerNorm.weight\n",
      "esm.encoder.layer.5.attention.LayerNorm.bias\n",
      "esm.encoder.layer.5.attention.LayerNorm.weight\n",
      "esm.encoder.layer.5.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.5.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.5.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.5.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.5.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.5.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.5.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.5.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.5.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.5.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.5.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.5.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.5.output.dense.base_layer.bias\n",
      "esm.encoder.layer.5.output.dense.base_layer.weight\n",
      "esm.encoder.layer.5.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.5.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.6.LayerNorm.bias\n",
      "esm.encoder.layer.6.LayerNorm.weight\n",
      "esm.encoder.layer.6.attention.LayerNorm.bias\n",
      "esm.encoder.layer.6.attention.LayerNorm.weight\n",
      "esm.encoder.layer.6.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.6.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.6.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.6.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.6.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.6.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.6.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.6.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.6.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.6.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.6.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.6.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.6.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.6.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.6.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.6.output.dense.base_layer.bias\n",
      "esm.encoder.layer.6.output.dense.base_layer.weight\n",
      "esm.encoder.layer.6.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.6.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.7.LayerNorm.bias\n",
      "esm.encoder.layer.7.LayerNorm.weight\n",
      "esm.encoder.layer.7.attention.LayerNorm.bias\n",
      "esm.encoder.layer.7.attention.LayerNorm.weight\n",
      "esm.encoder.layer.7.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.7.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.7.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.7.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.7.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.7.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.7.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.7.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.7.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.7.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.7.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.7.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.7.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.7.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.7.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.7.output.dense.base_layer.bias\n",
      "esm.encoder.layer.7.output.dense.base_layer.weight\n",
      "esm.encoder.layer.7.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.7.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.8.LayerNorm.bias\n",
      "esm.encoder.layer.8.LayerNorm.weight\n",
      "esm.encoder.layer.8.attention.LayerNorm.bias\n",
      "esm.encoder.layer.8.attention.LayerNorm.weight\n",
      "esm.encoder.layer.8.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.8.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.8.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.8.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.8.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.8.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.8.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.8.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.8.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.8.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.8.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.8.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.8.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.8.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.8.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.8.output.dense.base_layer.bias\n",
      "esm.encoder.layer.8.output.dense.base_layer.weight\n",
      "esm.encoder.layer.8.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.8.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.9.LayerNorm.bias\n",
      "esm.encoder.layer.9.LayerNorm.weight\n",
      "esm.encoder.layer.9.attention.LayerNorm.bias\n",
      "esm.encoder.layer.9.attention.LayerNorm.weight\n",
      "esm.encoder.layer.9.attention.output.dense.base_layer.bias\n",
      "esm.encoder.layer.9.attention.output.dense.base_layer.weight\n",
      "esm.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      "esm.encoder.layer.9.attention.self.key.base_layer.bias\n",
      "esm.encoder.layer.9.attention.self.key.base_layer.weight\n",
      "esm.encoder.layer.9.attention.self.key.lora_A.default.weight\n",
      "esm.encoder.layer.9.attention.self.key.lora_B.default.weight\n",
      "esm.encoder.layer.9.attention.self.query.base_layer.bias\n",
      "esm.encoder.layer.9.attention.self.query.base_layer.weight\n",
      "esm.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "esm.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "esm.encoder.layer.9.attention.self.rotary_embeddings.inv_freq\n",
      "esm.encoder.layer.9.attention.self.value.base_layer.bias\n",
      "esm.encoder.layer.9.attention.self.value.base_layer.weight\n",
      "esm.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "esm.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "esm.encoder.layer.9.intermediate.dense.base_layer.bias\n",
      "esm.encoder.layer.9.intermediate.dense.base_layer.weight\n",
      "esm.encoder.layer.9.intermediate.dense.lora_A.default.weight\n",
      "esm.encoder.layer.9.intermediate.dense.lora_B.default.weight\n",
      "esm.encoder.layer.9.output.dense.base_layer.bias\n",
      "esm.encoder.layer.9.output.dense.base_layer.weight\n",
      "esm.encoder.layer.9.output.dense.lora_A.default.weight\n",
      "esm.encoder.layer.9.output.dense.lora_B.default.weight\n",
      "lm_head.decoder.bias\n",
      "lm_head.decoder.weight\n",
      "lm_head.dense.base_layer.bias\n",
      "lm_head.dense.base_layer.weight\n",
      "lm_head.dense.lora_A.default.weight\n",
      "lm_head.dense.lora_B.default.weight\n",
      "lm_head.layer_norm.bias\n",
      "lm_head.layer_norm.weight\n",
      "pooler.dense.base_layer.bias\n",
      "pooler.dense.base_layer.weight\n",
      "pooler.dense.lora_A.default.weight\n",
      "pooler.dense.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "for key in state_dict.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained('Synthyra/ESMplusplus_small', trust_remote_code=True)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "sequences = ['MPRTEIN', 'MSEQWENCE']\n",
    "tokenized = tokenizer(sequences, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 0, 32, 31]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['affinity', 'binding']\n"
     ]
    }
   ],
   "source": [
    "subtypes = ['affinity', 'binding']\n",
    "\n",
    "f'{[subtype for subtype in subtypes]}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
